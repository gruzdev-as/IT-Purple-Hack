{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook contains some developments that were not included in the final version of the project\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport gc\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score, make_scorer\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV, KFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport xgboost as xgb\nfrom xgboost import plot_importance\n\nfrom hyperopt import fmin, tpe, hp, space_eval\nimport warnings\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom catboost import CatBoostClassifier, Pool\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature distribution**","metadata":{}},{"cell_type":"code","source":"numerical_features = df.select_dtypes(include=['float64', 'int64']).columns\nfor feature in numerical_features:\n    plt.figure(figsize=(10, 6))\n    sns.histplot(df[feature], kde=True)\n    plt.title(f'Distribution of {feature}')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Search and removal of multicollinear features**\n","metadata":{}},{"cell_type":"markdown","source":"First method - VIF [Variance Inflation Factor] ","metadata":{}},{"cell_type":"code","source":"class ReduceVIF(BaseEstimator, TransformerMixin):\n\n    def __init__(self, thresh=10, impute=True, impute_strategy='median'):\n        self.thresh = thresh\n\n        if impute:\n            self.imputer = SimpleImputer(strategy=impute_strategy)\n\n    def fit(self, X, y=None):\n        print('ReduceVIF fit')\n        if hasattr(self, 'imputer'):\n            self.imputer.fit(X)\n        return self\n\n    def transform(self, X, y=None):\n        print('ReduceVIF transform')\n        columns = X.columns.tolist()\n        if hasattr(self, 'imputer'):\n            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n        return ReduceVIF.calculate_vif(X, self.thresh)\n\n    @staticmethod\n    def calculate_vif(X, thresh=10):\n        dropped = True\n        while dropped:\n            variables = X.columns\n            dropped = False\n            vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]\n\n            max_vif = max(vif)\n            if max_vif > thresh:\n                maxloc = vif.index(max_vif)\n                print(f'Dropping {X.columns[maxloc]} with vif={max_vif}')\n                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n                dropped = True\n        return X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataframe is too large, so processing in chunks","metadata":{}},{"cell_type":"code","source":"def process_dataframe_in_chunks(df, n=10):\n    chunks = [df.iloc[:, i:i+n] for i in range(0, df.shape[1], n)]\n    processed_chunks = [transformer.fit_transform(chunk, y) for chunk in chunks]\n    return pd.concat(processed_chunks, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = process_dataframe_in_chunks(df, 10) # example of dataframe processing","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PCA [Principal Component Analysis]","metadata":{}},{"cell_type":"code","source":"df = df.drop(['feature642', 'feature756'], axis=1)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n\ndf_test['dummy'] = 'test'\ndf = pd.concat([df_train, df_test], axis=0, sort=False )\ndf = df.reset_index()\ndf = df.drop('index', axis=1)\n\ndef PCA_change(df, cols, n_components, rand_seed=4):\n    pca = PCA(n_components=n_components, random_state=rand_seed)\n\n    principalComponents = pca.fit_transform(df[cols])\n\n    principalDf = pd.DataFrame(principalComponents)\n\n    df.drop(cols, axis=1, inplace=True)\n\n    df = pd.concat([df, principalDf], axis=1)\n    \n    return df\n\nmas_v = df_train.columns[3:]\n\nfor col in mas_v:\n    df[col] = df[col].fillna((df[col].min() - 2))\n    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n\ndf = PCA_change(df, mas_v, n_components=100) # \n\ndf_train, df_test = df[df['dummy'] != 'test'], df[df['dummy'] == 'test'].drop('dummy', axis=1)\n\nX_train, y_train = df_train.drop(['id', 'target', 'sample_ml_new', 'isChurn'], axis=1), df_train['target']\nX_test, y_test = df_test.drop(['id', 'target', 'sample_ml_new'], axis=1), df_test['target']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**XGBoost test**","metadata":{}},{"cell_type":"code","source":"def objective(params):\n    time1 = time.time()\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'subsample': \"{:.2f}\".format(params['subsample']),\n        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 7\n    count=1\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n    tss = TimeSeriesSplit(n_splits=FOLDS)\n    y_preds = np.zeros(sample_submission.shape[0])\n    y_oof = np.zeros(X_train.shape[0])\n    score_mean = 0\n    for tr_idx, val_idx in tss.split(X_train, y_train):\n        clf = xgb.XGBClassifier(\n            n_estimators=600, random_state=4, verbose=True,\n            tree_method='gpu_hist', \n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr)\n\n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n        \n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean / FOLDS)\n\n\nspace = {\n    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n    'gamma': hp.uniform('gamma', 0.01, .7),\n    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=27)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = space_eval(space, best)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = {'bagging_fraction': 0.7955230034197366, 'colsample_bytree': 0.46278254431111865, 'feature_fraction': 0.5443072652214971, \n               'gamma': 0.5543303427555978, 'learning_rate': 0.01167276021142852, 'max_depth': 9, 'min_child_samples': 120, \n               'num_leaves': 230, 'reg_alpha': 0.3468832526697093, 'reg_lambda': 0.17207846996353005, 'subsample': 0.4}\n\nfrom xgboost import XGBClassifier\n\nclf = xgb.XGBClassifier(\n    n_estimators=300,\n    **best_params,\n    tree_method='gpu_hist'\n)\n\nclf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Importances**","metadata":{}},{"cell_type":"code","source":"feature_important = clf.get_booster().get_score(importance_type=\"weight\")\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n\n# Top 30 features\ndata.head(30)\n\nfav = data.head(30).reset_index()['index'].tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Shit Test**","metadata":{}},{"cell_type":"code","source":"names_lst = []\n\naucs_train_lst = []\naccuracy_train_lst = []\nprecision_train_lst = []\nrecall_train_lst = []\nf1_train_lst = []\n\naucs_test_lst = []\naccuracy_test_lst = []\nprecision_test_lst = []\nrecall_test_lst = []\nf1_test_lst = []\n\ndef build_measure_model(models):\n    plt.figure(figsize=(12,6))\n\n    for name, model, X_train, y_train, X_test, y_test in models:\n        \n        names_lst.append(name)\n\n        model.fit(X_train, y_train)\n        \n        y_train_pred = model.predict_proba(X_train)\n        y_test_pred = model.predict_proba(X_test)\n\n        pred_train = y_train_pred[:, 1]\n        pred_train_binary = (pred_train >= 0.1)\n        \n        pred_test = y_test_pred[:, 1]\n        pred_test_binary = (pred_test >= 0.1)\n\n        Accuracy_train = metrics.accuracy_score(y_train, pred_train_binary)\n        accuracy_train_lst.append(Accuracy_train)\n        \n        Accuracy_test = metrics.accuracy_score(y_test, pred_test_binary)\n        accuracy_test_lst.append(Accuracy_test)\n\n        Aucs_train = metrics.roc_auc_score(y_train, pred_train)\n        aucs_train_lst.append(Aucs_train)\n        \n        Aucs_test = metrics.roc_auc_score(y_test, pred_test)\n        aucs_test_lst.append(Aucs_test)\n\n        PrecisionScore_train = metrics.precision_score(y_train, pred_train_binary)\n        precision_train_lst.append(PrecisionScore_train)\n        \n        PrecisionScore_test = metrics.precision_score(y_test, pred_test_binary)\n        precision_test_lst.append(PrecisionScore_test)\n\n        RecallScore_train = metrics.recall_score(y_train, pred_train_binary)\n        recall_train_lst.append(RecallScore_train)\n        \n        RecallScore_test = metrics.recall_score(y_test, pred_test_binary)\n        recall_test_lst.append(RecallScore_test)\n\n        F1Score_train = metrics.f1_score(y_train, pred_train_binary)\n        f1_train_lst.append(F1Score_train)\n        \n        F1Score_test = metrics.f1_score(y_test, pred_test_binary)\n        f1_test_lst.append(F1Score_test)\n\n        cnf_matrix = metrics.confusion_matrix(y_test, pred_test_binary)\n\n        print(\"Model Name :\", name)\n        \n        print('Train Accuracy :{0:0.5f}'.format(Accuracy_train)) \n        print('Test Accuracy :{0:0.5f}'.format(Accuracy_test))\n        \n        print('Train AUC : {0:0.5f}'.format(Aucs_train))\n        print('Test AUC : {0:0.5f}'.format(Aucs_test))\n        \n        print('Train Precision : {0:0.5f}'.format(PrecisionScore_train))\n        print('Test Precision : {0:0.5f}'.format(PrecisionScore_test))\n        \n        print('Train Recall : {0:0.5f}'.format(RecallScore_train))\n        print('Test Recall : {0:0.5f}'.format(RecallScore_test))\n        \n        print('Train F1 : {0:0.5f}'.format(F1Score_train))\n        print('Test F1 : {0:0.5f}'.format(F1Score_test))\n        \n        print('Confusion Matrix : \\n', cnf_matrix)\n        print(\"\\n\")\n\n        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_test_pred)\n        auc = metrics.roc_auc_score(y_test, y_test_pred)\n        plt.plot(fpr,tpr,linewidth=2, label=name + \", auc=\"+str(auc))\n\n    plt.legend(loc=4)\n    plt.plot([0,1], [0,1], 'k--' )\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adasyn = ADASYN(random_state=42)\nX_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ros = RandomOverSampler(random_state=42)\nX_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rus = RandomUnderSampler(random_state=42)\nX_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistic Regression","metadata":{}},{"cell_type":"code","source":"LRmodels = []\n\nLRmodels.append(('LR imbalance', LogisticRegression(solver='liblinear', multi_class='ovr'), X_train, y_train, X_test, y_test))\nLRmodels.append(('LR Undersampling', LogisticRegression(solver='liblinear', multi_class='ovr'),X_train_rus, y_train_rus, X_test, y_test))\nLRmodels.append(('LR Oversampling', LogisticRegression(solver='liblinear', multi_class='ovr'), X_train_ros, y_train_ros, X_test, y_test))\nLRmodels.append(('LR SMOTE', LogisticRegression(solver='liblinear', multi_class='ovr'), X_train_smote, y_train_smote, X_test, y_test))\nLRmodels.append(('LR ADASYN', LogisticRegression(solver='liblinear', multi_class='ovr'), X_train_adasyn, y_train_adasyn, X_test, y_test))\n\nbuild_measure_model(LRmodels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CatBoost","metadata":{}},{"cell_type":"code","source":"CBmodels = []\n\ncb = CatBoostClassifier()\n\nCBmodels.append(('CB imbalance', cb, X_train, y_train, X_test,y_test))\nCBmodels.append(('CB Undersampling', cb, X_train_rus, y_train_rus, X_test, y_test))\nCBmodels.append(('CB Oversampling', cb, X_train_ros, y_train_ros, X_test, y_test))\nCBmodels.append(('CB SMOTE', cb, X_train_smote, y_train_smote, X_test, y_test))\nCBmodels.append(('CB ADASYN', cb, X_train_adasyn, y_train_adasyn, X_test, y_test))\n\n\nbuild_measure_model(CBmodels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {'Model': names_lst,\n       'Accuracy_Test': accuracy_test_lst,\n       'AUC_Test': aucs_test_lst,\n       'PrecisionScore_Test': precision_test_lst,\n       'RecallScore_Test': recall_test_lst,\n       'F1Score_Test': f1_test_lst}\n\nprint(\"Performance measures of various classifiers: \\n\")\nperformance_df = pd.DataFrame(data) \nperformance_df.sort_values(['F1Score_Test', 'RecallScore_Test', 'AUC_Test'], ascending=False)","metadata":{},"execution_count":null,"outputs":[]}]}